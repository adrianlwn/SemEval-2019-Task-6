{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from os import mkdir\n",
    "from os.path import join, isfile, isdir, exists\n",
    "import bcolz\n",
    "import pickle \n",
    "import emoji\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from pattern.en import spelling\n",
    "from tqdm import tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedings : GloVe\n",
    "This Class Loads the GloVe Embeding, processes it, and create a word embedding given the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe_embedding(object):\n",
    "    def __init__(self,dim_vect = 25 ):\n",
    "        ########## VARIABLES ##########\n",
    "        self.dim_vect = dim_vect\n",
    "\n",
    "        # Defining variables for GloVe: \n",
    "        self.words = []\n",
    "        self.word2idx = {}\n",
    "        self.glove_dict = {}\n",
    "        \n",
    "        ########## LOADING GLOVE DATA ##########\n",
    "        \n",
    "        # Defining path for GloVe Data : \n",
    "        self.path = join('..','data','glove') # Path of glove\n",
    "        self.path_glove = join(self.path,'glove.twitter.27B.'+str(dim_vect))\n",
    "        if not(isdir(self.path_glove)):\n",
    "            mkdir(self.path_glove)\n",
    "        self.path_vec_original = join(self.path,'glove.twitter.27B.'+str(dim_vect)+'d.txt') # Path of glove original vectors\n",
    "        self.path_vec_save = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.vectors.dat')  # Path of glove saved vectors\n",
    "        self.path_words = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.words.pkl')\n",
    "        self.path_word2idx = join(self.path_glove,'glove.twitter.27B.'+str(dim_vect)+'d.word2idx.pkl')\n",
    "                \n",
    "        if not(isdir(self.path_vec_save) and isfile(self.path_words) and isfile(self.path_word2idx)) : \n",
    "            # If files are allready processed, just load them\n",
    "            print('---- Processing the GloVe files : ',end='')\n",
    "            self.process_GloVe()\n",
    "            print('Done')\n",
    "            \n",
    "        # Load the wordvec files\n",
    "        print('---- Loading the processed GloVe files : ',end='')\n",
    "        self.load_GloVe()\n",
    "        print('Done')\n",
    "        \n",
    "        ########## TORCH EMBEDDING ##########\n",
    "        \n",
    "        # Defining variables for our Embedding:\n",
    "        self.size_vocab = len(self.words)\n",
    "        \n",
    "        # Creating the Pytorch Embedding Layer : \n",
    "        print('---- Creating the Pytorch Embedding Layer  : ',end='')\n",
    "        self.emb_layer = nn.Embedding(self.size_vocab, self.dim_vect)\n",
    "        self.create_emb_layer(non_trainable=True)\n",
    "        print('Done')\n",
    "\n",
    "               \n",
    "    def process_GloVe(self):\n",
    "        ''' Processes the GloVe Dataset - Saves files'''\n",
    "        words = []\n",
    "        word2idx = {}\n",
    "        \n",
    "        vectors = bcolz.carray(np.zeros(1) , rootdir=self.path_vec_save , mode='w' ) # defining vector saved\n",
    "        \n",
    "        # Adding Padding vector : \n",
    "        word2idx['<pad>'] = 0\n",
    "        words.append('<pad>')\n",
    "        #vect = np.random.normal(scale=0.6, size=(self.dim_vect , )) # random padding vect\n",
    "        vect = np.zeros((self.dim_vect , )) # 0's padding vect. \n",
    "        vectors.append(vect)\n",
    "        \n",
    "        idx = 1\n",
    "        with open(self.path_vec_original, 'rb') as f:\n",
    "            for l in f:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                words.append(word)\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "                vect = np.array(line[1:]).astype(np.float)\n",
    "                vectors.append(vect)\n",
    "                \n",
    "\n",
    "        vectors = bcolz.carray(vectors[:].reshape((-1, self.dim_vect)), rootdir=self.path_vec_save, mode='w')\n",
    "\n",
    "        vectors.flush()\n",
    "        pickle.dump(words, open(self.path_words, 'wb'))\n",
    "        pickle.dump(word2idx, open(self.path_word2idx, 'wb'))\n",
    "        \n",
    "    def load_GloVe(self):\n",
    "        ''' Loads previously processed dataset'''\n",
    "        \n",
    "        vectors = bcolz.open(self.path_vec_save)[:]\n",
    "        \n",
    "        self.words = pickle.load(open(self.path_words, 'rb'))\n",
    "        self.word2idx = pickle.load(open(self.path_word2idx, 'rb'))\n",
    "        \n",
    "        self.glove_dict = {w: vectors[self.word2idx[w]] for w in self.words}\n",
    "        self.emb_matrix = torch.Tensor(vectors)\n",
    "    \n",
    "    def create_emb_layer(self, non_trainable=True):\n",
    "        self.emb_layer.load_state_dict({'weight': self.emb_matrix})\n",
    "        if non_trainable:\n",
    "            self.emb_layer.weight.requires_grad = False\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Loading the processed GloVe files : Done\n",
      "---- Creating the Pytorch Embedding Layer  : Done\n"
     ]
    }
   ],
   "source": [
    "myEmbedding = GloVe_embedding(dim_vect=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "This Class Loads the Tweet Dataset, Cleans it. It also enables the loading for the training and testing. \n",
    "TODO : Loading for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTweetDataset(Dataset):\n",
    "    ''' \n",
    "    Pytorch Dataset for the Test set. \n",
    "    initialisation : - data : training pandas dataframe\n",
    "                     - subtask : subtask we are working on {'subtask_a', 'subtask_b', 'subtask_c', }\n",
    "                     - balanced : if we balance the dataset by oversampling it in the smallest classes\n",
    "    '''\n",
    "    def __init__(self,data, subtask):\n",
    "        self.id = data.index.tolist()\n",
    "        self.token = data.token.tolist()\n",
    "        self.token_id = data.token_id.tolist()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.token_id[index]), torch.FloatTensor([self.id[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token) \n",
    "\n",
    "        \n",
    "    \n",
    "class TweetDataset(Dataset):\n",
    "    ''' \n",
    "    Pytorch Dataset for the Training set. \n",
    "    initialisation : - data : training pandas dataframe\n",
    "                     - subtask : subtask we are working on {'subtask_a', 'subtask_b', 'subtask_c', }\n",
    "                     - balanced : if we balance the dataset by oversampling it in the smallest classes\n",
    "    '''\n",
    "    def __init__(self,data,subtask):        \n",
    "        # Save in lists the ids, labels, label_id, token, and token_id . \n",
    "        self.id = data.index.tolist()\n",
    "        self.label_id = data[subtask].tolist()\n",
    "        self.token = data.token.tolist()\n",
    "        self.token_id = data.token_id.tolist()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.token_id[index]), torch.FloatTensor([self.label_id[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandling(object):\n",
    "    def __init__(self, embedding, pValid):\n",
    "        print('-- Data Handling : ')\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.defineClasses()\n",
    "\n",
    "        # All the Text Data path\n",
    "        self.definePath()\n",
    "        \n",
    "        self.data = {}\n",
    "        \n",
    "        processed_ = True\n",
    "        for f in self.path:\n",
    "            processed_ = processed_ and isfile(self.path_clean[f])\n",
    "        \n",
    "        if  not(processed_) : \n",
    "            ### PROCESSING OF THE ORIGINAL DATASET\n",
    "            # Load, Clean and Tokenize the Datasets\n",
    "            print('---- Load, Clean and Tokensize Dataset : ',end='')\n",
    "            self.inital_dataload()\n",
    "            print('Done')\n",
    "            \n",
    "            # Compute List of All words in the datasets\n",
    "            print('---- Finalize tokenized words and translation to id : ',end='')\n",
    "            self.compute_wordlist()\n",
    "            self.token2id()\n",
    "            print('Done')\n",
    "\n",
    "            # Add Embedding and correct clean the words not in embedding : \n",
    "            print('---- Adapt Dataset for Embedding : ',end='')\n",
    "            self.adaptDataset()\n",
    "            print('Done')\n",
    "\n",
    "            # Save the Cleaned Datasets\n",
    "            print('---- Saving all tokenized words : ',end='')\n",
    "            self.save_cleanDataset()\n",
    "            print('Done')\n",
    "        else : \n",
    "            # Save the Cleaned Datasets\n",
    "            print('---- Load the Clean Adapted Dataset : ',end='')\n",
    "            self.load_cleanDataset()\n",
    "            \n",
    "            # Compute List of All words in the datasets\n",
    "            self.compute_wordlist()\n",
    "            print('Done')\n",
    "        \n",
    "        # Create Validation Set (split the test dataset) for every subtask\n",
    "        self.splitValidation(p=pValid)\n",
    "        self.prepareLabels()\n",
    "        \n",
    "        \n",
    "    def defineClasses(self):\n",
    "        ''' Function that defines the classes labels and id per subtask '''\n",
    "        self.classes_dict = {}\n",
    "        self.classes_dict['subtask_a'] = {'NOT' : 0 ,'OFF' : 1}\n",
    "        self.classes_dict['subtask_b'] = {'UNT' : 0 ,'TIN' : 1}\n",
    "        self.classes_dict['subtask_c'] = {'IND' : 0 ,'OTH' : 1, 'GRP' : 2}\n",
    "        \n",
    "    def definePath(self):\n",
    "        ''' Function that defines all the paths of the datasets. '''\n",
    "        self.path = {}\n",
    "        self.path_clean = {}\n",
    "        \n",
    "        self.path['train'] = join('..','data','start-kit','training-v1','offenseval-training-v1.tsv')\n",
    "        self.path_clean['train'] = join('..','data','start-kit','training-v1','clean-offenseval-training-v1.tsv')\n",
    "        \n",
    "        self.path['subtask_a'] = join('..','data','Test A Release','testset-taska.tsv')\n",
    "        self.path_clean['subtask_a'] = join('..','data','Test A Release','clean-testset-taska.tsv')\n",
    "        \n",
    "        self.path['subtask_b'] = join('..','data','Test B Release','testset-taskb.tsv')\n",
    "        self.path_clean['subtask_b'] = join('..','data','Test B Release','clean-testset-taskb.tsv')\n",
    "        \n",
    "        self.path['subtask_c'] = join('..','data','Test C Release','test_set_taskc.tsv')\n",
    "        self.path_clean['subtask_c'] = join('..','data','Test C Release','clean-test_set_taskc.tsv')\n",
    "        \n",
    "    def getDataset(self, dataT='train',subtask='subtask_a',balanced = True):\n",
    "        ''' Returns the pytorch Dataset\n",
    "            - file : {'train','test','validation'}\n",
    "            - subtask : {'subtask_a','subtask_b','subtask_c'} '''\n",
    "        \n",
    "            \n",
    "        if dataT == 'train':\n",
    "            if balanced : \n",
    "                data_train = self.balanceData(self.data[dataT][subtask],subtask)\n",
    "            else : \n",
    "                data_train = self.data[dataT][subtask]\n",
    "            dataset = TweetDataset(data_train, subtask)\n",
    "        elif dataT == 'validation':\n",
    "            dataset = TweetDataset(self.data[dataT][subtask], subtask)\n",
    "        elif dataT == 'test':\n",
    "            dataset = TestTweetDataset(self.data[subtask], subtask)\n",
    "            \n",
    "        return dataset\n",
    "    \n",
    "    def token2id(self):\n",
    "        ''' Function that translates the list of tokens into a list of token id of the embedding.\n",
    "            Adds a new 'token_id' column to the dataframe '''\n",
    "        for f in self.path : \n",
    "            def token2id_x(x):\n",
    "                \n",
    "                return [self.embedding.word2idx[k] for k in x if k in self.embedding.words]\n",
    "            self.data[f]['token_id'] = self.data[f]['token'].apply(lambda x : token2id_x(x))\n",
    "\n",
    "    def save_cleanDataset(self):\n",
    "        ''' Saves at the defined path the cleaned dataset '''\n",
    "        for f in self.path : \n",
    "            self.data[f].to_csv(self.path_clean[f])\n",
    "        \n",
    "    def load_cleanDataset(self):\n",
    "        ''' Loads at the defined path the cleaned dataset '''\n",
    "        for f in self.path : \n",
    "            self.data[f] = pd.read_csv(self.path_clean[f],index_col='id')\n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : ast.literal_eval(x))\n",
    "            self.data[f]['token_id'] = self.data[f]['token_id'].apply(lambda x : ast.literal_eval(x))\n",
    "           \n",
    "               \n",
    "    def adaptDataset(self):\n",
    "        ''' Function that finds all the words which are not in the embedding and tries to \n",
    "            correct them with the pattern.en package by taking the most probable replacement.\n",
    "            If the suggested word in very unlikely, the word is removed from the tweets. \n",
    "        '''\n",
    "        # Find all words wich are not in the Embedding :\n",
    "        missing_words = []\n",
    "        for i, word in enumerate(self.all_words) :\n",
    "            if self.embedding.word2idx.get(word) == None : \n",
    "                missing_words.append(word)\n",
    "        \n",
    "        # Correct if possible the missing_words : \n",
    "        ### We use theshold over which we correct the word. Under which we discard the word\n",
    "        t = 0.5 # threshold\n",
    "        rejected_words = []\n",
    "        corrected_words = {}\n",
    "        for word in tqdm(missing_words) : \n",
    "            suggestion, prob = spelling.suggest(word)[0]\n",
    "            if prob < t : \n",
    "                rejected_words.append(word)\n",
    "            else : \n",
    "                corrected_words[word] = suggestion\n",
    "        \n",
    "        # Modify the Original Datasets with those corrected_words : \n",
    "        for f in self.path : \n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : [corrected_words.get(k,k) for k in x])\n",
    "            self.data[f]['token'] = self.data[f]['token'].apply(lambda x : [k for k in x if k not in rejected_words ])\n",
    "        nb_rejected = len(rejected_words)\n",
    "        nb_corrected = len(corrected_words)\n",
    "        nb_vocab = len(self.embedding.glove_dict)\n",
    "        p_rejected = 100* nb_rejected / nb_vocab\n",
    "        p_corrected = 100* nb_corrected / nb_vocab\n",
    "        print('---- Words removed   : {0:} / {1:.2f} - {2:} %'.format(nb_rejected,nb_vocab,p_rejected))\n",
    "        print('---- Words corrected : {0:} / {1:.2f} - {2:} %'.format(nb_corrected,nb_vocab,p_corrected))\n",
    "        \n",
    "    def inital_dataload(self):\n",
    "        for f in self.path : \n",
    "            self.data[f] = pd.read_table(self.path[f],index_col='id')\n",
    "            self.data[f]['token'] = self.data[f]['tweet'].apply(lambda x : self.clean_tweet(x))\n",
    "            \n",
    "    def compute_wordlist(self):\n",
    "        self.all_words_freq = {}\n",
    "        self.all_words = []\n",
    "        \n",
    "        for f in self.data : \n",
    "            for i in range(len(self.data[f])):\n",
    "                for e in self.data[f].iloc[i].token:\n",
    "                    self.all_words_freq[e] = 1 + self.all_words_freq.get(e,0)\n",
    "        self.all_words = list(self.all_words_freq.keys())\n",
    "        \n",
    "    def splitValidation(self,p):\n",
    "        ''' Creates the validation set by  taking p % of the train dataset '''\n",
    "        data = self.data['train'].copy()\n",
    "        self.data['train'] = {}\n",
    "        self.data['validation'] = {}\n",
    "\n",
    "        for subtask in self.classes_dict: # per subtask\n",
    "            self.data['train'][subtask] = pd.DataFrame()\n",
    "            self.data['validation'][subtask]= pd.DataFrame()\n",
    "            for label in self.classes_dict[subtask]: #per label in this subtask \n",
    "                data_label =  data[data[subtask]==label]\n",
    "                self.data['train'][subtask] = self.data['train'][subtask].append(data.loc[data_label.index])\n",
    "                nb_valid = int(len(data_label)*p)\n",
    "                # Select randmoly (without repetition) the indexes of the selected vaidation tweets\n",
    "                index_valid = np.random.choice(data_label.index,(nb_valid,),replace=False)\n",
    "                # Add the the selected validation tweets to the new dataframe\n",
    "                self.data['validation'][subtask] = self.data['validation'][subtask].append(self.data['train'][subtask].loc[index_valid,:])\n",
    "                # Drop the selected validation tweets from the training set\n",
    "                self.data['train'][subtask] = self.data['train'][subtask].drop(index = index_valid)\n",
    "                \n",
    "    def prepareLabels(self) : \n",
    "        ''' Transform the labels into classes id '''\n",
    "        for subtask in self.classes_dict: # per subtask\n",
    "            self.data['validation'][subtask][subtask] =self.data['validation'][subtask][subtask].apply(lambda x : self.classes_dict[subtask][x])  \n",
    "            self.data['train'][subtask][subtask] = self.data['train'][subtask][subtask].apply(lambda x : self.classes_dict[subtask][x])  \n",
    "\n",
    "    def balanceData(self,data,subtask):\n",
    "        ''' Augments the Data given in input in order to balance the dataset'''\n",
    "        class_size = {}\n",
    "        for label in self.classes_dict[subtask]:\n",
    "            class_size[label] = len(data[data[subtask]==self.classes_dict[subtask][label]])\n",
    "        largest_class = max(class_size, key=class_size.get)\n",
    "        print('---- Augmenting the Data : ')\n",
    "        print('Before Augmentation : ',class_size)\n",
    "\n",
    "        for label in self.classes_dict[subtask]:  \n",
    "            if label != largest_class:\n",
    "                id_list = data[data[subtask]==self.classes_dict[subtask][label]].index\n",
    "                nb_augmentation = class_size[largest_class] - class_size[label]\n",
    "                id_augmentation = np.random.choice(id_list, (nb_augmentation,))\n",
    "                data = data.append(data.loc[id_augmentation,:])\n",
    "        # Check if it went well\n",
    "        for label in self.classes_dict[subtask]:\n",
    "            class_size[label] = len(data[data[subtask]==self.classes_dict[subtask][label]])\n",
    "        \n",
    "        print('After Augmentation : ',class_size)\n",
    "        return data\n",
    "    \n",
    "    def clean_tweet(self,text):\n",
    "        ''' Function that is applied to every to tweet in the dataset '''\n",
    "        \n",
    "        # =========== TEXT ===========\n",
    "        # Replace @USER by <user>\n",
    "        text = re.compile(r'@USER').sub(r'<user>',text)\n",
    "\n",
    "        # Replace URL by <url>\n",
    "        text = re.compile(r'URL').sub(r'<url>',text)\n",
    "\n",
    "        # Remove numbers :\n",
    "        text = re.compile(r'[0-9]+').sub(r' ',text)\n",
    "\n",
    "        # Remove some special characters\n",
    "        text = re.compile(r'([\\xa0_\\{\\}\\[\\]¬¨‚Ä¢$,:;/@#|\\^*%().~`‚Äù\"‚Äú-])').sub(r' ',text) \n",
    "\n",
    "        # Space the special characters with white spaces\n",
    "        text = re.compile(r'([$&+,:;=?@#|\\'.^*()%!\"‚Äô‚Äú-])').sub(r' \\1 ',text)\n",
    "        \n",
    "        # Replace some special characters : \n",
    "        replace_dict = {r'&' : 'and' , \n",
    "                        r'\\+' : 'plus'}\n",
    "        for cha in replace_dict:\n",
    "            text = re.compile(str(cha)).sub(str(replace_dict[cha]),text)\n",
    "            \n",
    "        # Handle Emoji : translate some and delete the others\n",
    "        text = self.handle_emoji(text)\n",
    "        \n",
    "        # Word delengthening : \n",
    "        text = re.compile(r'(.)\\1{3,}').sub(r'\\1\\1',text)\n",
    "\n",
    "        # Cut the words with caps in them : \n",
    "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)\n",
    "        text = re.compile(r'([a-z]+|[A-Z]+|[A-Z][a-z]+)([A-Z][a-z]+)').sub(r'\\1 \\2',text)        \n",
    "        # =========== TOKENS ===========\n",
    "        # TOKENIZE \n",
    "        text = text.split(' ')\n",
    "\n",
    "        # Remove white spaces tokens\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != ' ']\n",
    "\n",
    "        # Remove empty tokens\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != '']\n",
    "\n",
    "        # Remove repetition in tokens (!!! => !)\n",
    "        text = [text[i] for i in range(len(text)) if text[i] != text[i-1]]\n",
    "\n",
    "        #  Handle the ALL CAPS Tweets \n",
    "        ### if ratio of caps in the word > 75% add allcaps tag <allcaps>\n",
    "        caps_r = np.mean([text[i].isupper() for i in range(len(text))])\n",
    "        if caps_r > 0.6 : \n",
    "            text.append('<allcaps>')\n",
    "\n",
    "        # Lower Case : \n",
    "        text = [text[i].lower() for i in range(len(text))]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def handle_emoji(self,text):\n",
    "        # Dictionnary of \"important\" emojis : \n",
    "        emoji_dict =  {'‚ô•Ô∏è': ' love ',\n",
    "                       '‚ù§Ô∏è' : ' love ',\n",
    "                       '‚ù§' : ' love ',\n",
    "                       'üòò' : ' kisses ',\n",
    "                      'üò≠' : ' cry ',\n",
    "                      'üí™' : ' strong ',\n",
    "                      'üåç' : ' earth ',\n",
    "                      'üí∞' : ' money ',\n",
    "                      'üëç' : ' ok ',\n",
    "                       'üëå' : ' ok ',\n",
    "                      'üò°' : ' angry ',\n",
    "                      'üçÜ' : ' dick ',\n",
    "                      'ü§£' : ' haha ',\n",
    "                      'üòÇ' : ' haha ',\n",
    "                      'üñï' : ' fuck you '}\n",
    "\n",
    "        for cha in emoji_dict:\n",
    "            text = re.compile(str(cha)).sub(str(emoji_dict[cha]),text)\n",
    "        # Remove ALL emojis\n",
    "        text = emoji.get_emoji_regexp().sub(r' ',text) \n",
    "        text = re.compile(\"([\\U0001f3fb-\\U0001f3ff])\").sub(r'',text) \n",
    "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r'',text) \n",
    "        text = re.compile(\"(\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff])\").sub(r'',text)\n",
    "\n",
    "        # Add Space between  the Emoji Expressions : \n",
    "        text = re.compile(\"([\\U00010000-\\U0010ffff])\").sub(r' \\1 ',text) \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Data Handling : \n",
      "---- Load the Clean Adapted Dataset : Done\n"
     ]
    }
   ],
   "source": [
    "mydata = DataHandling(myEmbedding, pValid=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = mydata.getDataset(dataT='validation',subtask='subtask_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier \n",
    "Set of Classes used as classifier for the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification NN : \n",
    "class FFNN2(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, hidden_dim , num_classes ,embedding_dim):\n",
    "        print('------ Creating FFNN : ',end='')\n",
    "        \n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim[0])\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1]) \n",
    "        self.fc3 = nn.Linear(hidden_dim[1], num_classes) \n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Activation Layers\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Sigmoid()\n",
    "        print('Done')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        # we average the embeddings of words in a sentence\n",
    "        \n",
    "        non_zero_nb = (x!=0).sum(1,keepdim=True)\n",
    "        #print(x.shape, non_zero_nb,embedded.sum(1).shape)\n",
    "        averaged = embedded.sum(1) / non_zero_nb.float()\n",
    "        #averaged = embedded.mean(1)\n",
    "        # (batch size, max sent length, embedding dim) to (batch size, embedding dim)\n",
    "\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        ''' Returns the loss function best associated with the model'''\n",
    "        return nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification NN : \n",
    "class FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, hidden_dim , num_classes ,embedding_dim):\n",
    "        print('------ Creating FFNN : ',end='')\n",
    "        \n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes) \n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "        # Activation Layers\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Sigmoid()\n",
    "        print('Done')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        # we average the embeddings of words in a sentence\n",
    "        \n",
    "        non_zero_nb = (x!=0).sum(1,keepdim=True)\n",
    "        #print(x.shape, non_zero_nb,embedded.sum(1).shape)\n",
    "        averaged = embedded.sum(1) / non_zero_nb.float()\n",
    "        #averaged = embedded.mean(1)\n",
    "        # (batch size, max sent length, embedding dim) to (batch size, embedding dim)\n",
    "\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        ''' Returns the loss function best associated with the model'''\n",
    "        return nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification NN : \n",
    "class FFNN_multiDim(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, hidden_dim , num_classes ,embedding_dim):\n",
    "        print('------ Creating FFNN : ',end='')\n",
    "        \n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes) \n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "        # Activation Layers\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.LogSoftmax(dim=num_classes)\n",
    "        print('Done')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        # we average the embeddings of words in a sentence\n",
    "        \n",
    "        non_zero_nb = (x!=0).sum(1,keepdim=True)\n",
    "        #print(x.shape, non_zero_nb,embedded.sum(1).shape)\n",
    "        averaged = embedded.sum(1) / non_zero_nb.float()\n",
    "        #averaged = embedded.mean(1)\n",
    "        # (batch size, max sent length, embedding dim) to (batch size, embedding dim)\n",
    "\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        ''' Returns the loss function best associated with the model'''\n",
    "        return nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad Data Method : Used with the pytorch DataLoader in order to pad the length of the tweets by batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_tweet(batch):\n",
    "    '''\n",
    "    Pad Data Method : Used with the pytorch DataLoader in order to pad the length of the tweets by batch. \n",
    "    args: \n",
    "        batch - List of elements ( x , label )\n",
    "    return \n",
    "        batch - Padded ( list(x) , list(label))\n",
    "    \n",
    "    '''\n",
    "    batch = list(zip(*batch))\n",
    "    max_len = max([len(t) for t in batch[0]])\n",
    "    batch[0] = torch.stack([pad_tensor(vec=t, pad=max_len, dim=0) for t in batch[0]],dim=0)\n",
    "    batch[1] = torch.stack(batch[1])\n",
    "    return batch[0] , batch[1]\n",
    "\n",
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = pad - vec.size(dim)\n",
    "    return torch.cat([vec, torch.zeros(*pad_size,dtype=torch.long)], dim=dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Class : Trainer\n",
    "Main Class for the loading, training, testing etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffensiveClassifier(object):\n",
    "    ''' Main Class for the loading, training, testing etc ...'''\n",
    "    def __init__(self,subtask='subtask_a', dim_vect=25,cType='FFNN',pValid = 0.15):\n",
    "        \n",
    "        self.dim_vect = dim_vect\n",
    "        \n",
    "        self.subtask = subtask\n",
    "        \n",
    "\n",
    "        # Loading the GloVe Embedding and Torch Formating of this Embedding\n",
    "        self.GloVe = GloVe_embedding(dim_vect= dim_vect )\n",
    "        self.embedding = self.GloVe.emb_layer\n",
    "        \n",
    "        # Loading the Data Handler : \n",
    "        self.dataHandler = DataHandling(self.GloVe,pValid=pValid)\n",
    "        \n",
    "        # Retrieving Training DataSet (pytorch)\n",
    "        self.train_set = self.dataHandler.getDataset('train',subtask,balanced=True)\n",
    "        \n",
    "        # Retrieving the Validation Set (pytorch)\n",
    "        self.valid_set = self.dataHandler.getDataset('validation',subtask)\n",
    "\n",
    "        # Retrieving Test DataSet (pytorch)\n",
    "        self.test_set = self.dataHandler.getDataset('test',subtask)\n",
    "        \n",
    "        # Usefull info on classes : \n",
    "        self.class2id  = self.dataHandler.classes_dict[self.subtask]\n",
    "        self.id2class = dict(zip(self.class2id.values(),self.class2id.keys()))\n",
    "        self.nb_class = len(self.class2id)\n",
    "\n",
    "        \n",
    "        # Classification : \n",
    "        if cType == 'FFNN':\n",
    "            # Creating the Neural Network\n",
    "            self.model = FFNN(self.embedding, 100 , 1, self.dim_vect)\n",
    "        elif cType == 'FFNN2':\n",
    "            # Creating the Neural Network\n",
    "            self.model = FFNN2(self.embedding, [100,20], 1, self.dim_vect)\n",
    "        elif cType == 'FFNN_multiDim' :\n",
    "            self.model = FFNN_multiDim(self.embedding, 100 , self.nb_class, self.dim_vect)\n",
    "        elif cType == 'RNN':\n",
    "            \n",
    "            pass\n",
    "        \n",
    "    \n",
    "    def train( self, nb_epochs, lr=0.01, batch_size = 1000 ):\n",
    "        \n",
    "        self.train_generator = DataLoader(self.train_set, batch_size=batch_size,collate_fn=padding_tweet, shuffle=True)\n",
    "\n",
    "        optimizer = optim.RMSprop(self.model.parameters(), lr=lr)#, weight_decay=0.005)\n",
    "        \n",
    "        loss_fn = self.model.loss_fn()\n",
    "\n",
    "        for epoch in range(nb_epochs):\n",
    "            i_batch = 0\n",
    "            epoch_correct = 0\n",
    "            self.model.train() \n",
    "            \n",
    "            for tokens, target  in self.train_generator :\n",
    "                i_batch += 1\n",
    "                target = target.float()\n",
    "                tokens = tokens.long()\n",
    "                #to ensure the dropout (exlained later) is \"turned on\" while training\n",
    "                #good practice to include even if do not use here\n",
    "                self.model.train()\n",
    "\n",
    "                #we zero the gradients as they are not removed automatically\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                predictions = self.model(tokens)\n",
    "                \n",
    "                loss = loss_fn(predictions, target)\n",
    "                acc, correct = self.accuracy(predictions, target)\n",
    "                \n",
    "                #calculate the gradient of each parameter\n",
    "                loss.backward()\n",
    "                \n",
    "                #update the parameters using the gradients and optimizer algorithm \n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss = loss.item()\n",
    "\n",
    "                if i_batch % (nb_epochs -1 )  ==0:\n",
    "                    print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.3f} | Train Acc: {acc*100:.2f}%')\n",
    "            \n",
    "            self.validation(batch_size = 50)\n",
    "            \n",
    "    def validation(self, batch_size = 1000):\n",
    "        all_correct = 0\n",
    "        self.validation_generator = DataLoader(self.valid_set, batch_size=batch_size, collate_fn=padding_tweet, shuffle=True)\n",
    "        loss_fn = self.model.loss_fn()\n",
    "        nb_valid = len(taskAclassifier.valid_set)\n",
    "        self.model.eval()  # set model to evaluation mode\n",
    "        all_prediction = []\n",
    "        all_target = []\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for tokens, target  in self.validation_generator :\n",
    "                \n",
    "                \n",
    "                predictions = self.model(tokens)   \n",
    "                \n",
    "                loss = loss_fn(predictions, target)\n",
    "                \n",
    "                all_prediction.extend(predictions.view(-1,).tolist())\n",
    "                all_target.extend(target.view(-1,).tolist())\n",
    "        accuracy, CM, stats_df = self.results(all_prediction,all_target)\n",
    "        \n",
    "        print(f'|+| Validation Accuracy : {100*accuracy:.2f} %')\n",
    "        print(stats_df)\n",
    "        print(CM)\n",
    "            \n",
    "            \n",
    "\n",
    "    def test(self):\n",
    "        ''' \n",
    "            Test Function : Tests the Network on the Test Data of the Subtask and Saves in a file\n",
    "        '''\n",
    "        test_result = pd.DataFrame()\n",
    "        self.test_generator = DataLoader(self.test_set,collate_fn= padding_tweet )\n",
    "        nb_valid = len(taskAclassifier.valid_set)\n",
    "        self.model.eval()  # set model to evaluation mode\n",
    "        with torch.no_grad(): \n",
    "            for tweet, id_tweet  in self.test_generator :\n",
    "                predictions = id2class[int(np.round(int(self.model(tweet))))]\n",
    "                test_result.append({'index' : id_tweet , 'prediction' : predictions})\n",
    "                print(test_result)\n",
    "\n",
    "    def accuracy(self, output, target):\n",
    "        target = np.array(target.tolist()).astype(int)\n",
    "        output = np.array(torch.round(output).tolist()).astype(int)\n",
    "        correct = (output == target)\n",
    "        accuracy = correct.sum()/len(correct)\n",
    "        return accuracy, correct\n",
    "        \n",
    "    def results(self, output, target ):\n",
    "        target = np.array(target).astype(int)\n",
    "        output = np.round(output).astype(int)\n",
    "        correct = (output == target)\n",
    "        accuracy = correct.sum()/len(correct)\n",
    "        \n",
    "        CM = np.zeros((self.nb_class,self.nb_class))\n",
    "        for i in range(len(output)): \n",
    "            CM[target[i], output[i]] += 1\n",
    "        \n",
    "        # compute the interesting stats on the classification : \n",
    "        stats_df = self.stats(CM)\n",
    "\n",
    "        return (accuracy, CM, stats_df)\n",
    "\n",
    "    def stats(self,CM):   \n",
    "        n_class = self.nb_class\n",
    "    \n",
    "        stats = {}\n",
    "        stats['precision'] = {} \n",
    "        stats['recall'] = {} \n",
    "        stats['accuracy'] = {} \n",
    "        stats['f1-measure'] = {} \n",
    "\n",
    "        for t in range(self.nb_class):\n",
    "            tp = CM[t,t]\n",
    "            tn = np.sum([CM[i,i] for i in range(self.nb_class) if i != t])\n",
    "\n",
    "            fp = np.sum([CM[i,t] for i in range(self.nb_class) if i != t])\n",
    "            fn = np.sum([CM[t,i] for i in range(self.nb_class) if i != t])\n",
    "\n",
    "            # compute accuracy per class :\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "            # compute recall per class\n",
    "            recall = tp /(tp + fn)\n",
    "\n",
    "            # compute precision per class \n",
    "            precision = tp / (tp + fp)\n",
    "\n",
    "            # compute F1-measure per class \n",
    "            f1 = 2*tp / (2* tp + fp + fn)\n",
    "\n",
    "            # saving stats : \n",
    "\n",
    "            stats['precision'][t] = precision\n",
    "            stats['recall'][t] = recall\n",
    "            stats['accuracy'][t] = accuracy\n",
    "            stats['f1-measure'][t] = f1\n",
    "\n",
    "        # Compute Average classification rate\n",
    "        stats = pd.DataFrame(stats).transpose()\n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Loading the processed GloVe files : Done\n",
      "---- Creating the Pytorch Embedding Layer  : Done\n",
      "-- Data Handling : \n",
      "---- Load the Clean Adapted Dataset : Done\n",
      "---- Augmenting the Data : \n",
      "Before Augmentation :  {'NOT': 7072, 'OFF': 3520}\n",
      "After Augmentation :  {'NOT': 7072, 'OFF': 7072}\n",
      "------ Creating FFNN : Done\n"
     ]
    }
   ],
   "source": [
    "taskAclassifier = OffensiveClassifier(subtask='subtask_a', dim_vect=100, cType='FFNN',pValid = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|+| Validation Accuracy : 63.29 %\n",
      "                   0         1\n",
      "precision   0.685289  0.408000\n",
      "recall      0.832579  0.231818\n",
      "accuracy    0.632931  0.632931\n",
      "f1-measure  0.751788  0.295652\n",
      "[[1472.  296.]\n",
      " [ 676.  204.]]\n",
      "|+| Validation Accuracy : 33.69 %\n",
      "                   0         1\n",
      "precision   0.657895  0.332184\n",
      "recall      0.014140  0.985227\n",
      "accuracy    0.336858  0.336858\n",
      "f1-measure  0.027685  0.496848\n",
      "[[  25. 1743.]\n",
      " [  13.  867.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrian/anaconda3/envs/ic_std/lib/python3.6/site-packages/ipykernel/__main__.py:174: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|+| Validation Accuracy : 66.77 %\n",
      "                   0         1\n",
      "precision   0.667674       NaN\n",
      "recall      1.000000  0.000000\n",
      "accuracy    0.667674  0.667674\n",
      "f1-measure  0.800725  0.000000\n",
      "[[1768.    0.]\n",
      " [ 880.    0.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 61.90 %\n",
      "                   0         1\n",
      "precision   0.693130  0.405564\n",
      "recall      0.770362  0.314773\n",
      "accuracy    0.618958  0.618958\n",
      "f1-measure  0.729708  0.354447\n",
      "[[1362.  406.]\n",
      " [ 603.  277.]]\n",
      "|+| Validation Accuracy : 63.37 %\n",
      "                   0         1\n",
      "precision   0.687676  0.413793\n",
      "recall      0.826923  0.245455\n",
      "accuracy    0.633686  0.633686\n",
      "f1-measure  0.750899  0.308131\n",
      "[[1462.  306.]\n",
      " [ 664.  216.]]\n",
      "|+| Validation Accuracy : 37.84 %\n",
      "                   0         1\n",
      "precision   0.824468  0.344309\n",
      "recall      0.087670  0.962500\n",
      "accuracy    0.378399  0.378399\n",
      "f1-measure  0.158487  0.507186\n",
      "[[ 155. 1613.]\n",
      " [  33.  847.]]\n",
      "|+| Validation Accuracy : 56.34 %\n",
      "                   0         1\n",
      "precision   0.704000  0.379791\n",
      "recall      0.597285  0.495455\n",
      "accuracy    0.563444  0.563444\n",
      "f1-measure  0.646267  0.429980\n",
      "[[1056.  712.]\n",
      " [ 444.  436.]]\n",
      "|+| Validation Accuracy : 48.19 %\n",
      "                   0         1\n",
      "precision   0.721477  0.359749\n",
      "recall      0.364819  0.717045\n",
      "accuracy    0.481873  0.481873\n",
      "f1-measure  0.484598  0.479119\n",
      "[[ 645. 1123.]\n",
      " [ 249.  631.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 64.35 %\n",
      "                   0         1\n",
      "precision   0.678819  0.406977\n",
      "recall      0.884615  0.159091\n",
      "accuracy    0.643505  0.643505\n",
      "f1-measure  0.768173  0.228758\n",
      "[[1564.  204.]\n",
      " [ 740.  140.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 51.10 %\n",
      "                   0         1\n",
      "precision   0.705117  0.361204\n",
      "recall      0.459842  0.613636\n",
      "accuracy    0.510952  0.510952\n",
      "f1-measure  0.556659  0.454737\n",
      "[[813. 955.]\n",
      " [340. 540.]]\n",
      "|+| Validation Accuracy : 59.21 %\n",
      "                   0         1\n",
      "precision   0.689219  0.379518\n",
      "recall      0.708710  0.357955\n",
      "accuracy    0.592145  0.592145\n",
      "f1-measure  0.698829  0.368421\n",
      "[[1253.  515.]\n",
      " [ 565.  315.]]\n",
      "|+| Validation Accuracy : 51.32 %\n",
      "                   0         1\n",
      "precision   0.703483  0.360979\n",
      "recall      0.468326  0.603409\n",
      "accuracy    0.513218  0.513218\n",
      "f1-measure  0.562309  0.451723\n",
      "[[828. 940.]\n",
      " [349. 531.]]\n",
      "|+| Validation Accuracy : 50.08 %\n",
      "                   0         1\n",
      "precision   0.704963  0.358333\n",
      "recall      0.433824  0.635227\n",
      "accuracy    0.500755  0.500755\n",
      "f1-measure  0.537115  0.458197\n",
      "[[ 767. 1001.]\n",
      " [ 321.  559.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 42.98 %\n",
      "                   0         1\n",
      "precision   0.724739  0.348120\n",
      "recall      0.235294  0.820455\n",
      "accuracy    0.429758  0.429758\n",
      "f1-measure  0.355252  0.488829\n",
      "[[ 416. 1352.]\n",
      " [ 158.  722.]]\n",
      "|+| Validation Accuracy : 51.32 %\n",
      "                   0         1\n",
      "precision   0.701430  0.359836\n",
      "recall      0.471719  0.596591\n",
      "accuracy    0.513218  0.513218\n",
      "f1-measure  0.564085  0.448910\n",
      "[[834. 934.]\n",
      " [355. 525.]]\n",
      "|+| Validation Accuracy : 48.83 %\n",
      "                   0         1\n",
      "precision   0.706294  0.355798\n",
      "recall      0.399887  0.665909\n",
      "accuracy    0.488293  0.488293\n",
      "f1-measure  0.510654  0.463791\n",
      "[[ 707. 1061.]\n",
      " [ 294.  586.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 37.27 %\n",
      "                   0         1\n",
      "precision   0.723849  0.337900\n",
      "recall      0.097851  0.925000\n",
      "accuracy    0.372734  0.372734\n",
      "f1-measure  0.172397  0.494983\n",
      "[[ 173. 1595.]\n",
      " [  66.  814.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 45.85 %\n",
      "                   0         1\n",
      "precision   0.713555  0.351554\n",
      "recall      0.315611  0.745455\n",
      "accuracy    0.458459  0.458459\n",
      "f1-measure  0.437647  0.477786\n",
      "[[ 558. 1210.]\n",
      " [ 224.  656.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 59.29 %\n",
      "                   0         1\n",
      "precision   0.693603  0.385681\n",
      "recall      0.699095  0.379545\n",
      "accuracy    0.592900  0.592900\n",
      "f1-measure  0.696338  0.382589\n",
      "[[1236.  532.]\n",
      " [ 546.  334.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 51.70 %\n",
      "                   0         1\n",
      "precision   0.701235  0.360782\n",
      "recall      0.481900  0.587500\n",
      "accuracy    0.516994  0.516994\n",
      "f1-measure  0.571237  0.447038\n",
      "[[852. 916.]\n",
      " [363. 517.]]\n",
      "|+| Validation Accuracy : 66.05 %\n",
      "                   0         1\n",
      "precision   0.673177  0.431655\n",
      "recall      0.955317  0.068182\n",
      "accuracy    0.660498  0.660498\n",
      "f1-measure  0.789806  0.117763\n",
      "[[1689.   79.]\n",
      " [ 820.   60.]]\n",
      "|+| Validation Accuracy : 33.50 %\n",
      "                   0         1\n",
      "precision   0.705882  0.332573\n",
      "recall      0.006787  0.994318\n",
      "accuracy    0.334970  0.334970\n",
      "f1-measure  0.013445  0.498433\n",
      "[[  12. 1756.]\n",
      " [   5.  875.]]\n",
      "|+| Validation Accuracy : 66.31 %\n",
      "                   0         1\n",
      "precision   0.669112  0.396552\n",
      "recall      0.980204  0.026136\n",
      "accuracy    0.663142  0.663142\n",
      "f1-measure  0.795319  0.049041\n",
      "[[1733.   35.]\n",
      " [ 857.   23.]]\n",
      "|+| Validation Accuracy : 63.63 %\n",
      "                   0         1\n",
      "precision   0.678809  0.395466\n",
      "recall      0.864253  0.178409\n",
      "accuracy    0.636329  0.636329\n",
      "f1-measure  0.760388  0.245889\n",
      "[[1528.  240.]\n",
      " [ 723.  157.]]\n",
      "|+| Validation Accuracy : 33.31 %\n",
      "                   0         1\n",
      "precision   1.000000  0.332577\n",
      "recall      0.001131  1.000000\n",
      "accuracy    0.333082  0.333082\n",
      "f1-measure  0.002260  0.499149\n",
      "[[   2. 1766.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 66.09 %\n",
      "                   0         1\n",
      "precision   0.671260  0.416667\n",
      "recall      0.964367  0.051136\n",
      "accuracy    0.660876  0.660876\n",
      "f1-measure  0.791551  0.091093\n",
      "[[1705.   63.]\n",
      " [ 835.   45.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 51.66 %\n",
      "                   0         1\n",
      "precision   0.701987  0.361111\n",
      "recall      0.479638  0.590909\n",
      "accuracy    0.516616  0.516616\n",
      "f1-measure  0.569892  0.448276\n",
      "[[848. 920.]\n",
      " [360. 520.]]\n",
      "|+| Validation Accuracy : 60.39 %\n",
      "                   0         1\n",
      "precision   0.690312  0.388669\n",
      "recall      0.737557  0.335227\n",
      "accuracy    0.603852  0.603852\n",
      "f1-measure  0.713153  0.359976\n",
      "[[1304.  464.]\n",
      " [ 585.  295.]]\n",
      "|+| Validation Accuracy : 57.33 %\n",
      "                   0         1\n",
      "precision   0.689881  0.370868\n",
      "recall      0.655543  0.407955\n",
      "accuracy    0.573263  0.573263\n",
      "f1-measure  0.672274  0.388528\n",
      "[[1159.  609.]\n",
      " [ 521.  359.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 38.44 %\n",
      "                   0         1\n",
      "precision   0.712963  0.338640\n",
      "recall      0.130656  0.894318\n",
      "accuracy    0.384441  0.384441\n",
      "f1-measure  0.220841  0.491261\n",
      "[[ 231. 1537.]\n",
      " [  93.  787.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 41.81 %\n",
      "                   0         1\n",
      "precision   0.713748  0.343883\n",
      "recall      0.214367  0.827273\n",
      "accuracy    0.418051  0.418051\n",
      "f1-measure  0.329709  0.485819\n",
      "[[ 379. 1389.]\n",
      " [ 152.  728.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 64.31 %\n",
      "                   0         1\n",
      "precision   0.676685  0.398119\n",
      "recall      0.891403  0.144318\n",
      "accuracy    0.643127  0.643127\n",
      "f1-measure  0.769343  0.211843\n",
      "[[1576.  192.]\n",
      " [ 753.  127.]]\n",
      "|+| Validation Accuracy : 64.09 %\n",
      "                   0         1\n",
      "precision   0.681960  0.411911\n",
      "recall      0.865950  0.188636\n",
      "accuracy    0.640861  0.640861\n",
      "f1-measure  0.763020  0.258769\n",
      "[[1531.  237.]\n",
      " [ 714.  166.]]\n",
      "|+| Validation Accuracy : 49.70 %\n",
      "                   0         1\n",
      "precision   0.700737  0.355314\n",
      "recall      0.430430  0.630682\n",
      "accuracy    0.496979  0.496979\n",
      "f1-measure  0.533287  0.454545\n",
      "[[ 761. 1007.]\n",
      " [ 325.  555.]]\n",
      "|+| Validation Accuracy : 53.21 %\n",
      "                   0         1\n",
      "precision   0.702993  0.366543\n",
      "recall      0.518100  0.560227\n",
      "accuracy    0.532100  0.532100\n",
      "f1-measure  0.596548  0.443146\n",
      "[[916. 852.]\n",
      " [387. 493.]]\n",
      "|+| Validation Accuracy : 44.11 %\n",
      "                   0         1\n",
      "precision   0.724299  0.350449\n",
      "recall      0.263009  0.798864\n",
      "accuracy    0.441088  0.441088\n",
      "f1-measure  0.385892  0.487179\n",
      "[[ 465. 1303.]\n",
      " [ 177.  703.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 59.78 %\n",
      "                   0         1\n",
      "precision   0.687267  0.380026\n",
      "recall      0.729638  0.332955\n",
      "accuracy    0.597810  0.597810\n",
      "f1-measure  0.707819  0.354936\n",
      "[[1290.  478.]\n",
      " [ 587.  293.]]\n",
      "|+| Validation Accuracy : 65.82 %\n",
      "                   0         1\n",
      "precision   0.672255  0.412587\n",
      "recall      0.952489  0.067045\n",
      "accuracy    0.658233  0.658233\n",
      "f1-measure  0.788205  0.115347\n",
      "[[1684.   84.]\n",
      " [ 821.   59.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 65.90 %\n",
      "                   0         1\n",
      "precision   0.671968  0.413534\n",
      "recall      0.955882  0.062500\n",
      "accuracy    0.658988  0.658988\n",
      "f1-measure  0.789166  0.108588\n",
      "[[1690.   78.]\n",
      " [ 825.   55.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 34.14 %\n",
      "                   0         1\n",
      "precision   0.740000  0.333718\n",
      "recall      0.020928  0.985227\n",
      "accuracy    0.341390  0.341390\n",
      "f1-measure  0.040704  0.498562\n",
      "[[  37. 1731.]\n",
      " [  13.  867.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 46.26 %\n",
      "                   0         1\n",
      "precision   0.711138  0.351720\n",
      "recall      0.328620  0.731818\n",
      "accuracy    0.462613  0.462613\n",
      "f1-measure  0.449516  0.475101\n",
      "[[ 581. 1187.]\n",
      " [ 236.  644.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 60.91 %\n",
      "                   0         1\n",
      "precision   0.687085  0.387518\n",
      "recall      0.761312  0.303409\n",
      "accuracy    0.609139  0.609139\n",
      "f1-measure  0.722297  0.340344\n",
      "[[1346.  422.]\n",
      " [ 613.  267.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 63.52 %\n",
      "                   0         1\n",
      "precision   0.683776  0.407725\n",
      "recall      0.843891  0.215909\n",
      "accuracy    0.635196  0.635196\n",
      "f1-measure  0.755443  0.282318\n",
      "[[1492.  276.]\n",
      " [ 690.  190.]]\n",
      "|+| Validation Accuracy : 56.23 %\n",
      "                   0         1\n",
      "precision   0.699411  0.375558\n",
      "recall      0.604072  0.478409\n",
      "accuracy    0.562311  0.562311\n",
      "f1-measure  0.648255  0.420790\n",
      "[[1068.  700.]\n",
      " [ 459.  421.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 66.01 %\n",
      "                   0         1\n",
      "precision   0.667568  0.327586\n",
      "recall      0.977941  0.021591\n",
      "accuracy    0.660121  0.660121\n",
      "f1-measure  0.793483  0.040512\n",
      "[[1729.   39.]\n",
      " [ 861.   19.]]\n",
      "|+| Validation Accuracy : 57.82 %\n",
      "                   0         1\n",
      "precision   0.694561  0.378462\n",
      "recall      0.657240  0.419318\n",
      "accuracy    0.578172  0.578172\n",
      "f1-measure  0.675385  0.397844\n",
      "[[1162.  606.]\n",
      " [ 511.  369.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 33.23 %\n",
      "                   0         1\n",
      "precision        NaN  0.332326\n",
      "recall      0.000000  1.000000\n",
      "accuracy    0.332326  0.332326\n",
      "f1-measure  0.000000  0.498866\n",
      "[[   0. 1768.]\n",
      " [   0.  880.]]\n",
      "|+| Validation Accuracy : 65.45 %\n",
      "                   0         1\n",
      "precision   0.671630  0.392638\n",
      "recall      0.944005  0.072727\n",
      "accuracy    0.654456  0.654456\n",
      "f1-measure  0.784858  0.122723\n",
      "[[1669.   99.]\n",
      " [ 816.   64.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|+| Validation Accuracy : 65.22 %\n",
      "                   0         1\n",
      "precision   0.671527  0.385475\n",
      "recall      0.937783  0.078409\n",
      "accuracy    0.652190  0.652190\n",
      "f1-measure  0.782629  0.130312\n",
      "[[1658.  110.]\n",
      " [ 811.   69.]]\n",
      "|+| Validation Accuracy : 58.76 %\n",
      "                   0         1\n",
      "precision   0.683896  0.369136\n",
      "recall      0.710973  0.339773\n",
      "accuracy    0.587613  0.587613\n",
      "f1-measure  0.697171  0.353846\n",
      "[[1257.  511.]\n",
      " [ 581.  299.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-2e02ef378d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtaskAclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-0bc1fca324c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, nb_epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mi_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ic_std/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ic_std/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-afd6dfcdc7a4>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "taskAclassifier.train(500,lr=0.01,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can only append a Series if ignore_index=True or if the Series has a name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-e886d56e5ca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtaskAclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-273-1f34d0b4eda3>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_tweet\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_generator\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mtest_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mid_tweet\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ic_std/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6175\u001b[0m                 \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6177\u001b[0;31m                 raise TypeError('Can only append a Series if ignore_index=True'\n\u001b[0m\u001b[1;32m   6178\u001b[0m                                 ' or if the Series has a name')\n\u001b[1;32m   6179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can only append a Series if ignore_index=True or if the Series has a name"
     ]
    }
   ],
   "source": [
    "taskAclassifier.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 4])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(torch.LongTensor([1,2,4]).tolist()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = [[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.extend(((torch.LongTensor([1,2,4])).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 2, 4]), [1, 2, 4], <enumerate at 0x1cd5fe3dc8>, 1, 2, 4]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 3.])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round([1.1,2.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOT': 0, 'OFF': 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taskAclassifier.class2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 5.],\n",
       "        [5., 3., 2.],\n",
       "        [1., 8., 3.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "i = torch.FloatTensor([[1,1,5],[5,3,2],[1,8,3]])\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7668e-02, 1.7668e-02, 9.6466e-01],\n",
       "        [8.4379e-01, 1.1420e-01, 4.2010e-02],\n",
       "        [9.0496e-04, 9.9241e-01, 6.6868e-03]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = F.softmax(i,dim=1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2184, 0.2184, 0.5631],\n",
       "        [0.5180, 0.2497, 0.2323],\n",
       "        [0.2127, 0.5733, 0.2140]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = F.softmax(i,dim=1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2931, 0.2931, 0.4138],\n",
       "        [0.3974, 0.3039, 0.2987],\n",
       "        [0.2911, 0.4175, 0.2914]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = F.softmax(i,dim=1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3197, 0.3197, 0.3607],\n",
       "        [0.3550, 0.3233, 0.3216],\n",
       "        [0.3190, 0.3619, 0.3191]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = F.softmax(i,dim=1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3287, 0.3287, 0.3425],\n",
       "        [0.3406, 0.3300, 0.3294],\n",
       "        [0.3285, 0.3429, 0.3286]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = F.softmax(i,dim=1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3318, 0.3318, 0.3364],\n",
       "        [0.3358, 0.3322, 0.3320],\n",
       "        [0.3317, 0.3365, 0.3317]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = F.softmax(i,dim=1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ic_std]",
   "language": "python",
   "name": "conda-env-ic_std-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
